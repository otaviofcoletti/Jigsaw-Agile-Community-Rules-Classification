{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-13T04:10:04.745432Z",
     "iopub.status.busy": "2025-10-13T04:10:04.744684Z",
     "iopub.status.idle": "2025-10-13T04:10:04.863224Z",
     "shell.execute_reply": "2025-10-13T04:10:04.862442Z",
     "shell.execute_reply.started": "2025-10-13T04:10:04.745407Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/jigsaw/submission.csv\n",
      "/kaggle/input/jigsaw/train.csv\n",
      "/kaggle/input/jigsaw/test.csv\n",
      "/kaggle/input/wikipedia-toxic-comments/sample_submission.csv\n",
      "/kaggle/input/wikipedia-toxic-comments/test_labels.csv\n",
      "/kaggle/input/wikipedia-toxic-comments/train.csv\n",
      "/kaggle/input/wikipedia-toxic-comments/test.csv\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/rust_model.ot\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/config.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/README.md\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/tokenizer.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/tf_model.h5\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/data_config.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/train_script.py\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/tokenizer_config.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/sentence_bert_config.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/pytorch_model.bin\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/config_sentence_transformers.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/model.safetensors\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/modules.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/special_tokens_map.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.gitattributes\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/vocab.txt\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/1_Pooling/config.json\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/onnx/model.onnx\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/config\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/packed-refs\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/HEAD\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/index\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/description\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/info/exclude\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/refs/heads/main\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/refs/remotes/origin/HEAD\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/pre-merge-commit.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/post-merge\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/pre-push\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/prepare-commit-msg.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/update.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/pre-push.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/pre-rebase.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/pre-applypatch.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/post-commit\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/post-checkout\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/push-to-checkout.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/pre-commit.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/commit-msg.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/post-update.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/pre-receive.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/fsmonitor-watchman.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/hooks/applypatch-msg.sample\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/lfs/objects/53/aa/53aa51172d142c89d9012cce15ae4d6cc0ca6895895114379cacb4fab128d9db\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/lfs/objects/c3/a8/c3a85f238711653950f6a79ece63eb0ea93d76f6a6284be04019c53733baf256\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/lfs/objects/2d/98/2d98d96d278348988f2744e6445b8bc16d921c3f6e17c667362f3cb353007aea\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/lfs/objects/c0/1c/c01c3fb6f89996e15c1cb92ea3fa9cc3c76e9f385ed0b88ca71aebfad2be1547\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/lfs/objects/24/c0/24c06a7429b843d46e40c6b167122053921bf94dce2e5550ea5c07fabc597646\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/logs/HEAD\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/logs/refs/heads/main\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/logs/refs/remotes/origin/HEAD\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/c7/9f2b6a0cea6f4b564fed1938984bace9d30ff0\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/61/23922cd0a37e85554070835282693f77c2cdfd\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/bf/12db93776dfe464438f7aa49e5286dc1f6a1d3\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/2e/85f0eac205cf444bdf97ede4935603ca6a0416\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/6d/34772f5ca361021038b404fb913ec8dc0b1a5a\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/64/629dafb9c2f452be5264e1b39ca07703c285cb\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/ff/e79e29723a82be6e47be2645d46f5e6cb7d4c6\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/f0/e720b4416e21613c5f66bd2c80941a1d104e34\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/ac/bb28c8aa70f5503c85d6b90e8cd65606993a20\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/8b/3219a92973c328a8e22fadcfa821b5dc75636a\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/2c/fdf1ac2b3fc894d8239133174385c0baed9a17\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/38/d2c305133e9d034646e99db5533c6f36ebbafc\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/7d/bbc90392e2f80f3d3c277d6e90027e55de9125\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/e7/b0375001f109a6b8873d756ad4f7bbb15fbaa5\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/e4/ce9877abf3edfe10b0d82785e83bdcb973e22e\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/1a/310852cf8e58d22c5ebff537711d504ad4ad66\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/34/17920b056b9a3d4257fba7a8bae5c5ae2910d6\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/c2/1050a7ef692090620a6d037dd736908f9c7cf6\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/88/2b870dcd23cc8168e1e49fe6ef74b99d327416\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/cb/202bfe2e3c98645018a6d12f182a434c9d3e02\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/72/b987fd805cfa2b58c4c8c952b274a11bfd5a00\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/13/afc4e1fcb2e8688c8b565e612d78bf0cc0a203\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/99/7ea4ddfcbcd445473fc757249fc971a80337c9\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/7f/56ac753f54f5e3cc9aff3c5ab77b090bd699d3\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/fb/140275c155a9c7c5a3b3e0e77a9e839594a938\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/d4/9eff1e4a6f0e3ba5069f21c6d20aad156dd2c9\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/74/640519a949f5c678e1c29cfe92b2f0f5d41354\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/c6/aecd063146daa28b47cfc2b0ec031d17f27ec4\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/c4/360cc4f803de8692c8152724105929eb278b7d\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/d1/514c3162bbe87b343f565fadc62e6c06f04f03\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/3f/2fccdd494d71e84130e80d200194e53f030fc6\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/95/2a9b81c0bfd99800fabf352f69c7ccd46c5e43\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/91/7c37d89f3927e303b0f3f2e565a460604819de\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/71/7413c64de70e37b55cf53c9cdff0e2d331fac3\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/59/d594003bf59880a884c574bf88ef7555bb0202\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/59/0529fb2f5ca26d5fdbea2bb538d399ddcc79fd\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/fe/bd144c5f6a5614481e9998377f85003d4c848f\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/7c/3e117e015025bc605669cc11546708105b6f97\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/2b/f99854609538f90af03c921c41a0cbde670b2f\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/a4/ce9694439b000ffd9ef4fe8becf6f62621689e\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/1f/04b00315ed69102b46817616310699bd49e129\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/82/b47b6277499b8f17d139d0c651a6f961c06124\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/dd/d10b4b20ead19a8121a3d8b4369091fb408e1c\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/5f/dfc644dfd78c46a33c04abeedadf36e7c6d37b\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/5f/d10429389515d3e5cccdeda08cae5fea1ae82e\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/46/605decb5369335a3847c9f41bb0b896c07dd1a\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/46/66f08731cd88a74a36484a43f249a6bfede34b\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/a1/c250340fff88adddcf3cfeab206d898f5553b8\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/21/b29ab864793dc86d157902941b2ff4bbe2bbca\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/44/eb4044493a3c34bc6d7faae1a71ec76665ebc6\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/9a/1874cfc81fe03bde4dca6bde406fb80a6ddb29\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/63/daf3d0630eaec6401eed4a9bfb1e115bf2ba3e\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/ad/0563d758cca630a0ac9d344a42e14a05780583\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/37/46fd5f4cfd46ae64fc781df53e7cbb7849eb62\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/f7/2a19c9599caf1f8944a10d363150a5d36ff06c\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/8c/fec92309f5626a223304af2423e332f6d31887\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/9c/c46352c6e0729b6d9a3ce65e4603a16a63a9f9\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/b3/406917f3229edc5165ba222038d9bffe957a2f\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/29/368ef8bd008d7fb711a5968e4029dae93d9587\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/fd/1b291129c607e5d49799f87cb219b27f98acdf\n",
      "/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2/.git/objects/93/45a404b1c759ba192be71bfe32bc304f7a94e7\n",
      "/kaggle/input/jigsaw-agile-community-rules/sample_submission.csv\n",
      "/kaggle/input/jigsaw-agile-community-rules/train.csv\n",
      "/kaggle/input/jigsaw-agile-community-rules/test.csv\n",
      "/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1/config.json\n",
      "/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1/README.md\n",
      "/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1/tokenizer.json\n",
      "/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1/data_config.json\n",
      "/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1/train_script.py\n",
      "/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1/tokenizer_config.json\n",
      "/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1/sentence_bert_config.json\n",
      "/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1/pytorch_model.bin\n",
      "/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1/config_sentence_transformers.json\n",
      "/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1/model.safetensors\n",
      "/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1/modules.json\n",
      "/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1/special_tokens_map.json\n",
      "/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1/.gitattributes\n",
      "/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1/vocab.txt\n",
      "/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1/1_Pooling/config.json\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/config.json\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/README.md\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/tokenizer.json\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/tokenizer_config.json\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/sparse_linear.pt\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/sentence_bert_config.json\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/pytorch_model.bin\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/long.jpg\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/config_sentence_transformers.json\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/colbert_linear.pt\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/modules.json\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/special_tokens_map.json\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/.gitattributes\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/sentencepiece.bpe.model\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/1_Pooling/config.json\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/onnx/model.onnx\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/onnx/config.json\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/onnx/tokenizer.json\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/onnx/Constant_7_attr__value\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/onnx/tokenizer_config.json\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/onnx/special_tokens_map.json\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/onnx/model.onnx_data\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/onnx/sentencepiece.bpe.model\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/imgs/bm25.jpg\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/imgs/others.webp\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/imgs/miracl.jpg\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/imgs/long.jpg\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/imgs/mkqa.jpg\n",
      "/kaggle/input/bge-m3/transformers/m3/1/bge-m3/imgs/nqa.jpg\n",
      "/kaggle/input/qwen-3-embedding/transformers/4b/1/model.safetensors.index.json\n",
      "/kaggle/input/qwen-3-embedding/transformers/4b/1/config.json\n",
      "/kaggle/input/qwen-3-embedding/transformers/4b/1/merges.txt\n",
      "/kaggle/input/qwen-3-embedding/transformers/4b/1/model-00001-of-00002.safetensors\n",
      "/kaggle/input/qwen-3-embedding/transformers/4b/1/model-00002-of-00002.safetensors\n",
      "/kaggle/input/qwen-3-embedding/transformers/4b/1/README.md\n",
      "/kaggle/input/qwen-3-embedding/transformers/4b/1/tokenizer.json\n",
      "/kaggle/input/qwen-3-embedding/transformers/4b/1/vocab.json\n",
      "/kaggle/input/qwen-3-embedding/transformers/4b/1/tokenizer_config.json\n",
      "/kaggle/input/qwen-3-embedding/transformers/4b/1/generation_config.json\n",
      "/kaggle/input/qwen-3-embedding/transformers/0.6b/1/config.json\n",
      "/kaggle/input/qwen-3-embedding/transformers/0.6b/1/merges.txt\n",
      "/kaggle/input/qwen-3-embedding/transformers/0.6b/1/README.md\n",
      "/kaggle/input/qwen-3-embedding/transformers/0.6b/1/tokenizer.json\n",
      "/kaggle/input/qwen-3-embedding/transformers/0.6b/1/vocab.json\n",
      "/kaggle/input/qwen-3-embedding/transformers/0.6b/1/tokenizer_config.json\n",
      "/kaggle/input/qwen-3-embedding/transformers/0.6b/1/model.safetensors\n",
      "/kaggle/input/qwen-3-embedding/transformers/0.6b/1/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T04:10:04.864687Z",
     "iopub.status.busy": "2025-10-13T04:10:04.864473Z",
     "iopub.status.idle": "2025-10-13T04:10:04.870262Z",
     "shell.execute_reply": "2025-10-13T04:10:04.869482Z",
     "shell.execute_reply.started": "2025-10-13T04:10:04.864671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import lightgbm as lgb\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Set\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.utils import check_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T04:10:04.871683Z",
     "iopub.status.busy": "2025-10-13T04:10:04.871178Z",
     "iopub.status.idle": "2025-10-13T04:10:04.884879Z",
     "shell.execute_reply": "2025-10-13T04:10:04.884207Z",
     "shell.execute_reply.started": "2025-10-13T04:10:04.871658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Paths & Models ---\n",
    "TRAIN_PATH = \"/kaggle/input/jigsaw-agile-community-rules/train.csv\"\n",
    "TEST_PATH  = \"/kaggle/input/jigsaw-agile-community-rules/test.csv\"\n",
    "\n",
    "AUG_DATA_PATH = \"/kaggle/input/wikipediacomments/train.csv\"\n",
    "\n",
    "EMBED_MODEL_PATHS = {\n",
    "    'bge-m3': \"/kaggle/input/bge-m3/transformers/m3/1/bge-m3\",\n",
    "    'all-mpnet-base-v2': \"/kaggle/input/all-mpnet-base-v2/transformers/all-mpnet-base-v2/1\",\n",
    "    \"all-minilm-l6-v2\": \"/kaggle/input/all-minilm-l6-v2/transformers/default/1/all-MiniLM-L6-v2\",\n",
    "    'qwen-3': \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\",\n",
    "}\n",
    "\n",
    "# --- Runtime Parameters ---\n",
    "BATCH_SIZE = 32\n",
    "N_SPLITS = 5\n",
    "WEIGHT_POWER = 2.0\n",
    "OUT_ENSEMBLE_CSV = \"/kaggle/working/submission.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilidades e formatacao do dataset de data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T04:10:04.886040Z",
     "iopub.status.busy": "2025-10-13T04:10:04.885769Z",
     "iopub.status.idle": "2025-10-13T04:10:04.905719Z",
     "shell.execute_reply": "2025-10-13T04:10:04.904966Z",
     "shell.execute_reply.started": "2025-10-13T04:10:04.885988Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_and_format_toxic_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and formats the Wikipedia Toxic Comments dataset for augmentation.\n",
    "    \"\"\"\n",
    "    print(f\"Loading and formatting augmentation data from: {Path(path).name}\")\n",
    "    df_toxic = pd.read_csv(path)\n",
    "    \n",
    "    # Define the columns that represent different types of toxicity\n",
    "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "    # Create a single 'rule_violation' label: 1 if any toxic flag is set, 0 otherwise\n",
    "    df_toxic['rule_violation'] = df_toxic[label_cols].max(axis=1)\n",
    "\n",
    "    # Assign the specific rule this data augments\n",
    "    df_toxic['rule'] = \"No toxic or hostile behavior.\"\n",
    "\n",
    "    # Rename the 'comment_text' column to 'body'\n",
    "    df_toxic = df_toxic.rename(columns={'comment_text': 'body'})\n",
    "    \n",
    "    # Keep only the necessary columns\n",
    "    df_aug_toxic = df_toxic[['body', 'rule', 'rule_violation']]\n",
    "    print(f\"Formatted {len(df_aug_toxic)} rows for augmentation.\")\n",
    "    return df_aug_toxic\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def normalize_lc(s: str) -> str:\n",
    "    return normalize_text(s).lower()\n",
    "\n",
    "def auto_majority_vote_within_rule(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    tmp = df.copy()\n",
    "    tmp[\"norm_body\"] = tmp[\"body\"].map(normalize_lc)\n",
    "    g = tmp.groupby([\"rule\", \"norm_body\"])[\"rule_violation\"]\n",
    "    stats = g.agg([\"sum\", \"count\"]).rename(columns={\"sum\": \"pos\", \"count\": \"n\"})\n",
    "    stats[\"neg\"] = stats[\"n\"] - stats[\"pos\"]\n",
    "    stats[\"maj_label\"] = (stats[\"pos\"] >= stats[\"neg\"]).astype(int)\n",
    "    tmp = tmp.merge(stats[\"maj_label\"], left_on=[\"rule\", \"norm_body\"], right_index=True, how=\"left\")\n",
    "    tmp[\"rule_violation\"] = tmp[\"maj_label\"]\n",
    "    tmp = (tmp.sort_values([\"rule\", \"norm_body\"])\n",
    "              .drop_duplicates([\"rule\", \"norm_body\"], keep=\"first\")\n",
    "              .drop(columns=[\"maj_label\"]))\n",
    "    return tmp.drop(columns=[\"norm_body\"])\n",
    "\n",
    "def get_example_cols(df: pd.DataFrame) -> Tuple[List[str], List[str]]:\n",
    "    pos_cols = [c for c in df.columns if c.startswith(\"positive_example\")]\n",
    "    neg_cols = [c for c in df.columns if c.startswith(\"negative_example\")]\n",
    "    return pos_cols, neg_cols\n",
    "\n",
    "def build_rule_example_pools(df: pd.DataFrame) -> Dict[str, Dict[str, Set[str]]]:\n",
    "    pos_cols, neg_cols = get_example_cols(df)\n",
    "    pools: Dict[str, Dict[str, Set[str]]] = {}\n",
    "    for rule, g in df.groupby(\"rule\"):\n",
    "        pos_set, neg_set = set(), set()\n",
    "        for c in pos_cols:\n",
    "            if c in g:\n",
    "                pos_set.update(g[c].dropna().astype(str).map(str.strip).tolist())\n",
    "        for c in neg_cols:\n",
    "            if c in g:\n",
    "                neg_set.update(g[c].dropna().astype(str).map(str.strip).tolist())\n",
    "        pools[rule] = {\"pos\": {t for t in pos_set if t}, \"neg\": {t for t in neg_set if t}}\n",
    "    return pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T04:10:04.907870Z",
     "iopub.status.busy": "2025-10-13T04:10:04.907558Z",
     "iopub.status.idle": "2025-10-13T04:10:04.925821Z",
     "shell.execute_reply": "2025-10-13T04:10:04.925067Z",
     "shell.execute_reply.started": "2025-10-13T04:10:04.907851Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_text_index(train_df: pd.DataFrame, test_df: pd.DataFrame) -> List[str]:\n",
    "    texts: List[str] = []\n",
    "    texts.extend(train_df[\"body\"].fillna(\"\").tolist())\n",
    "    texts.extend(train_df[\"rule\"].fillna(\"\").tolist())\n",
    "    texts.extend(test_df[\"body\"].fillna(\"\").tolist())\n",
    "    texts.extend(test_df[\"rule\"].fillna(\"\").tolist())\n",
    "    \n",
    "    for df in [train_df, test_df]:\n",
    "        pos_cols, neg_cols = get_example_cols(df)\n",
    "        for c in pos_cols + neg_cols:\n",
    "            if c in df:\n",
    "                texts.extend(df[c].dropna().astype(str).map(str.strip).tolist())\n",
    "    \n",
    "    seen, uniq = set(), []\n",
    "    for t in texts:\n",
    "        t = str(t)\n",
    "        if t and t not in seen:\n",
    "            seen.add(t)\n",
    "            uniq.append(t)\n",
    "    return uniq\n",
    "\n",
    "def embed_all(texts: List[str], model: SentenceTransformer, batch_size: int):\n",
    "    embs = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    return {t: e.astype(np.float32) for t, e in zip(texts, embs)}\n",
    "\n",
    "def _sim_stats(vec: np.ndarray, mats: List[np.ndarray]) -> List[float]:\n",
    "    if vec is None or len(mats) == 0:\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    sims = cosine_similarity(vec[None, :], np.vstack(mats))[0]\n",
    "    return [float(sims.max()), float(sims.mean()), float(sims.min())]\n",
    "\n",
    "def row_features_single_model(\n",
    "    row: pd.Series,\n",
    "    emb_cache: Dict[str, np.ndarray],\n",
    "    rule_pools: Dict[str, Dict[str, Set[str]]]) -> np.ndarray:\n",
    "    \n",
    "    body = emb_cache.get(str(row[\"body\"]))\n",
    "    rule = emb_cache.get(str(row[\"rule\"]))\n",
    "    \n",
    "    rule_key = str(row[\"rule\"])\n",
    "    pos_txts = rule_pools.get(rule_key, {}).get(\"pos\", set())\n",
    "    neg_txts = rule_pools.get(rule_key, {}).get(\"neg\", set())\n",
    "\n",
    "    pos_embs = [emb_cache[t] for t in pos_txts if t in emb_cache]\n",
    "    neg_embs = [emb_cache[t] for t in neg_txts if t in emb_cache]\n",
    "\n",
    "    feats: List[float] = []\n",
    "    feats.append(float(np.dot(body, rule)) if (body is not None and rule is not None) else 0.0)\n",
    "    \n",
    "    pmax, pmean, pmin = _sim_stats(body, pos_embs)\n",
    "    nmax, nmean, nmin = _sim_stats(body, neg_embs)\n",
    "    \n",
    "    feats.extend([pmax, pmean, pmin, nmax, nmean, nmin, pmax - nmax, pmean - nmean])\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "def build_feature_matrix(\n",
    "    df: pd.DataFrame,\n",
    "    emb_caches: Dict[str, Dict[str, np.ndarray]],\n",
    "    rule_pools: Dict[str, Dict[str, Set[str]]],\n",
    "    model_names: List[str]) -> np.ndarray:\n",
    "    \n",
    "    rows = []\n",
    "    for i in range(len(df)):\n",
    "        row_series = df.iloc[i]\n",
    "        per_model_feats = [\n",
    "            row_features_single_model(row_series, emb_caches[mn], rule_pools)\n",
    "            for mn in model_names\n",
    "        ]\n",
    "        rows.append(np.hstack(per_model_feats))\n",
    "    return np.vstack(rows).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcao de definicao dos modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T04:10:04.927001Z",
     "iopub.status.busy": "2025-10-13T04:10:04.926642Z",
     "iopub.status.idle": "2025-10-13T04:10:04.943394Z",
     "shell.execute_reply": "2025-10-13T04:10:04.942696Z",
     "shell.execute_reply.started": "2025-10-13T04:10:04.926976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_estimator(kind: str, seed: int):\n",
    "    \"\"\"Factory to create different ML models.\"\"\"\n",
    "    \n",
    "    if kind == \"lr\":\n",
    "        # Logistic Regression: A robust and fast linear model.\n",
    "        print(\"  -> Creating LogisticRegression estimator\")\n",
    "        return LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            solver=\"lbfgs\",\n",
    "            class_weight=\"balanced\",\n",
    "            C=1.0,\n",
    "            n_jobs=-1,\n",
    "            random_state=seed,\n",
    "        )\n",
    "    \n",
    "    if kind == \"ridge\":\n",
    "        # Ridge Classifier: A linear model with L2 regularization, good for high-dimensional data.\n",
    "        # We wrap it with CalibratedClassifierCV to get probability outputs.\n",
    "        print(\"  -> Creating RidgeClassifier estimator\")\n",
    "        base = RidgeClassifier(class_weight=\"balanced\", random_state=seed)\n",
    "        calibrated = CalibratedClassifierCV(base, method=\"sigmoid\", cv=3)\n",
    "        return make_pipeline(StandardScaler(), calibrated)\n",
    "        \n",
    "    if kind == \"nb\":\n",
    "        # Gaussian Naive Bayes: Extremely fast, assumes features are normally distributed.\n",
    "        # A simple but often effective baseline.\n",
    "        print(\"  -> Creating GaussianNB estimator\")\n",
    "        return GaussianNB()\n",
    "\n",
    "    if kind == \"lgbm\":\n",
    "        # LightGBM: A fast, high-performance gradient boosting framework.\n",
    "        print(\"  -> Creating LGBMClassifier estimator\")\n",
    "        use_gpu = torch.cuda.is_available()\n",
    "        params = dict(\n",
    "            objective=\"binary\",\n",
    "            metric=\"auc\",\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            device=\"gpu\" if use_gpu else \"cpu\",\n",
    "        )\n",
    "        return lgb.LGBMClassifier(**params)\n",
    "\n",
    "    raise ValueError(f\"Unknown estimator kind: {kind}\")\n",
    "\n",
    "def make_balanced_weights(y):\n",
    "    counts = np.bincount(y)\n",
    "    n = len(y)\n",
    "    w = np.ones_like(y, dtype=np.float64)\n",
    "    for c in (0, 1):\n",
    "        if c < len(counts) and counts[c] > 0:\n",
    "            w[y == c] = n / (2.0 * counts[c])\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T04:10:04.944552Z",
     "iopub.status.busy": "2025-10-13T04:10:04.944241Z",
     "iopub.status.idle": "2025-10-13T04:10:04.963893Z",
     "shell.execute_reply": "2025-10-13T04:10:04.963229Z",
     "shell.execute_reply.started": "2025-10-13T04:10:04.944529Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_config(config_dict: dict, random_state: int):\n",
    "    \"\"\"\n",
    "    Runs a single configuration from data loading to model training.\n",
    "    \"\"\"\n",
    "    name = config_dict[\"name\"]\n",
    "    embed_keys = config_dict[\"embed_model_keys\"]\n",
    "    learners = config_dict[\"learners\"]\n",
    "    \n",
    "    print(f\"\\n{'='*25}\\nConfig: {name}\\nEmbeddings: {embed_keys}\\nLearners: {learners}\\n{'='*25}\\n\")\n",
    "    \n",
    "    # --- Data Loading & Augmentation ---\n",
    "    df_train_raw = pd.read_csv(TRAIN_PATH)\n",
    "    df_test = pd.read_csv(TEST_PATH)\n",
    "    \n",
    "    # ** INTEGRATED AUGMENTATION STEP **\n",
    "    if Path(AUG_DATA_PATH).exists():\n",
    "        df_aug_toxic = load_and_format_toxic_data(AUG_DATA_PATH)\n",
    "        df_train_raw = pd.concat([df_train_raw, df_aug_toxic], ignore_index=True)\n",
    "        print(f\"Training data augmented. New shape: {df_train_raw.shape}\")\n",
    "\n",
    "    # Clean and de-duplicate the combined training data\n",
    "    df_train = auto_majority_vote_within_rule(df_train_raw)\n",
    "    \n",
    "    train_pools = build_rule_example_pools(df_train)\n",
    "    test_pools = build_rule_example_pools(df_test)\n",
    "    \n",
    "    pred_pools = {k: v.copy() for k, v in train_pools.items()}\n",
    "    for rule, d in test_pools.items():\n",
    "        pred_pools.setdefault(rule, {\"pos\": set(), \"neg\": set()})\n",
    "        pred_pools[rule][\"pos\"].update(d.get(\"pos\", set()))\n",
    "        pred_pools[rule][\"neg\"].update(d.get(\"neg\", set()))\n",
    "\n",
    "    # --- Embedding ---\n",
    "    uniq_texts = build_text_index(df_train, df_test)\n",
    "    emb_caches = {}\n",
    "    for key in embed_keys:\n",
    "        model_path = EMBED_MODEL_PATHS[key]\n",
    "        print(f\"\\nLoading embedding model: {key}\")\n",
    "        model = SentenceTransformer(model_path, trust_remote_code=True)\n",
    "        emb_caches[key] = embed_all(uniq_texts, model, batch_size=BATCH_SIZE)\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # --- Feature Matrix Building ---\n",
    "    print(\"\\nBuilding feature matrices...\")\n",
    "    X_train = build_feature_matrix(df_train, emb_caches, train_pools, embed_keys)\n",
    "    y_train = df_train[\"rule_violation\"].astype(int).values\n",
    "    X_test = build_feature_matrix(df_test, emb_caches, pred_pools, embed_keys)\n",
    "    print(f\"Feature matrix shape: {X_train.shape}\")\n",
    "    \n",
    "    del emb_caches, uniq_texts\n",
    "    gc.collect()\n",
    "\n",
    "    # --- CV and Prediction Loop ---\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=random_state)\n",
    "    members = []\n",
    "\n",
    "    for learner_kind in learners:\n",
    "        print(f\"\\n--- Training Learner: {learner_kind.upper()} ---\")\n",
    "        oof_preds = np.zeros(len(X_train), dtype=float)\n",
    "        test_preds_folds = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "            X_tr, y_tr = X_train[train_idx], y_train[train_idx]\n",
    "            X_val, y_val = X_train[val_idx], y_train[val_idx]\n",
    "\n",
    "            estimator = make_estimator(learner_kind, random_state)\n",
    "            fit_params = {}\n",
    "            if learner_kind == \"lgbm\":\n",
    "                fit_params['eval_set'] = [(X_val, y_val)]\n",
    "                fit_params['callbacks'] = [lgb.early_stopping(50, verbose=False)]\n",
    "                \n",
    "            if learner_kind == \"ridge\":\n",
    "                 fit_params[f'{estimator.steps[-1][0]}__sample_weight'] = make_balanced_weights(y_tr)\n",
    "                    \n",
    "            estimator.fit(X_tr, y_tr, **fit_params)\n",
    "            \n",
    "            fold_preds = estimator.predict_proba(X_val)[:, 1]\n",
    "            oof_preds[val_idx] = fold_preds\n",
    "            test_preds_folds.append(estimator.predict_proba(X_test)[:, 1])\n",
    "            print(f\"  Fold {fold} AUC: {roc_auc_score(y_val, fold_preds):.5f}\")\n",
    "\n",
    "        oof_auc = roc_auc_score(y_train, oof_preds)\n",
    "        print(f\">> {name} [{learner_kind}] | Overall OOF AUC: {oof_auc:.5f}\")\n",
    "        \n",
    "        # Refit on full data\n",
    "        estimator_full = make_estimator(learner_kind, random_state)\n",
    "        fit_params_full = {}\n",
    "        if learner_kind == \"ridge\":\n",
    "            fit_params_full[f'{estimator_full.steps[-1][0]}__sample_weight'] = make_balanced_weights(y_train)\n",
    "\n",
    "        estimator_full.fit(X_train, y_train, **fit_params_full)\n",
    "        test_preds_full = estimator_full.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        members.append({\n",
    "            \"name\": f\"{name}_{learner_kind}\",\n",
    "            \"oof_auc\": oof_auc,\n",
    "            \"test_preds\": test_preds_full,\n",
    "        })\n",
    "    \n",
    "    return members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabela de Engenharia de Features\n",
    "\n",
    "| Característica      | Descrição                                                                                                                                                    |\n",
    "| :------------------ | :----------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `sim_body_rule`     | A similaridade de cosseno entre o embedding do comentário (`body`) e o da regra (`rule`). Mede o quão semanticamente próximo o comentário está do texto da regra. |\n",
    "| `pmax`              | A **máxima** similaridade de cosseno entre o comentário e todos os **exemplos positivos** (violações conhecidas) associados à regra.                               |\n",
    "| `pmean`             | A **média** da similaridade de cosseno entre o comentário e todos os **exemplos positivos** da regra.                                                            |\n",
    "| `pmin`              | A **mínima** similaridade de cosseno entre o comentário e todos os **exemplos positivos** da regra.                                                            |\n",
    "| `nmax`              | A **máxima** similaridade de cosseno entre o comentário e todos os **exemplos negativos** (não violações conhecidas) da regra.                             |\n",
    "| `nmean`             | A **média** da similaridade de cosseno entre o comentário e todos os **exemplos negativos** da regra.                                                            |\n",
    "| `nmin`              | A **mínima** similaridade de cosseno entre o comentário e todos os **exemplos negativos** da regra.                                                            |\n",
    "| `pmax - nmax`       | A **diferença** entre a similaridade máxima com exemplos positivos e a máxima com exemplos negativos.      |\n",
    "| `pmean - nmean`     | A **diferença** entre a similaridade média com exemplos positivos e a média com exemplos negativos.     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-13T04:10:04.964824Z",
     "iopub.status.busy": "2025-10-13T04:10:04.964563Z",
     "iopub.status.idle": "2025-10-13T04:11:27.874405Z",
     "shell.execute_reply": "2025-10-13T04:11:27.873727Z",
     "shell.execute_reply.started": "2025-10-13T04:10:04.964802Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================\n",
      "Config: config_A\n",
      "Embeddings: ['bge-m3', 'all-mpnet-base-v2', 'qwen-3']\n",
      "Learners: ['lr', 'lgbm']\n",
      "=========================\n",
      "\n",
      "\n",
      "Loading embedding model: bge-m3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8756e5f88cff410690247ee0d9459c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embedding model: all-mpnet-base-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0524982ff6924e45a0e5e59b57dfb56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading embedding model: qwen-3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c509184bdf4c64bbac73aff6def6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building feature matrices...\n",
      "Feature matrix shape: (1873, 27)\n",
      "\n",
      "--- Training Learner: LR ---\n",
      "  -> Creating LogisticRegression estimator\n",
      "  Fold 1 AUC: 0.93923\n",
      "  -> Creating LogisticRegression estimator\n",
      "  Fold 2 AUC: 0.91556\n",
      "  -> Creating LogisticRegression estimator\n",
      "  Fold 3 AUC: 0.93202\n",
      "  -> Creating LogisticRegression estimator\n",
      "  Fold 4 AUC: 0.92351\n",
      "  -> Creating LogisticRegression estimator\n",
      "  Fold 5 AUC: 0.91927\n",
      ">> config_A [lr] | Overall OOF AUC: 0.92499\n",
      "  -> Creating LogisticRegression estimator\n",
      "\n",
      "--- Training Learner: LGBM ---\n",
      "  -> Creating LGBMClassifier estimator\n",
      "  Fold 1 AUC: 0.94594\n",
      "  -> Creating LGBMClassifier estimator\n",
      "  Fold 2 AUC: 0.92922\n",
      "  -> Creating LGBMClassifier estimator\n",
      "  Fold 3 AUC: 0.93900\n",
      "  -> Creating LGBMClassifier estimator\n",
      "  Fold 4 AUC: 0.93879\n",
      "  -> Creating LGBMClassifier estimator\n",
      "  Fold 5 AUC: 0.92788\n",
      ">> config_A [lgbm] | Overall OOF AUC: 0.90955\n",
      "  -> Creating LGBMClassifier estimator\n",
      "\n",
      "=========================\n",
      "Config: config_B\n",
      "Embeddings: ['all-minilm-l6-v2']\n",
      "Learners: ['nb', 'ridge']\n",
      "=========================\n",
      "\n",
      "\n",
      "Loading embedding model: all-minilm-l6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b9c3c257214990b0e678ca67083fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building feature matrices...\n",
      "Feature matrix shape: (1873, 9)\n",
      "\n",
      "--- Training Learner: NB ---\n",
      "  -> Creating GaussianNB estimator\n",
      "  Fold 1 AUC: 0.92732\n",
      "  -> Creating GaussianNB estimator\n",
      "  Fold 2 AUC: 0.92920\n",
      "  -> Creating GaussianNB estimator\n",
      "  Fold 3 AUC: 0.92912\n",
      "  -> Creating GaussianNB estimator\n",
      "  Fold 4 AUC: 0.90699\n",
      "  -> Creating GaussianNB estimator\n",
      "  Fold 5 AUC: 0.93445\n",
      ">> config_B [nb] | Overall OOF AUC: 0.92517\n",
      "  -> Creating GaussianNB estimator\n",
      "\n",
      "--- Training Learner: RIDGE ---\n",
      "  -> Creating RidgeClassifier estimator\n",
      "  Fold 1 AUC: 0.92222\n",
      "  -> Creating RidgeClassifier estimator\n",
      "  Fold 2 AUC: 0.93348\n",
      "  -> Creating RidgeClassifier estimator\n",
      "  Fold 3 AUC: 0.92766\n",
      "  -> Creating RidgeClassifier estimator\n",
      "  Fold 4 AUC: 0.90782\n",
      "  -> Creating RidgeClassifier estimator\n",
      "  Fold 5 AUC: 0.93270\n",
      ">> config_B [ridge] | Overall OOF AUC: 0.92424\n",
      "  -> Creating RidgeClassifier estimator\n",
      "\n",
      "=== Final Ensemble Blending ===\n",
      "Member 'config_A_lr': OOF AUC=0.92499, Weight=0.2522\n",
      "Member 'config_A_lgbm': OOF AUC=0.90955, Weight=0.2438\n",
      "Member 'config_B_nb': OOF AUC=0.92517, Weight=0.2523\n",
      "Member 'config_B_ridge': OOF AUC=0.92424, Weight=0.2518\n",
      "\n",
      " Ensemble submission written to: /kaggle/working/submission.csv\n",
      "Submission Head:\n",
      "   row_id  rule_violation\n",
      "0    2029        0.141144\n",
      "1    2030        0.003154\n",
      "2    2031        0.992868\n",
      "3    2032        0.959717\n",
      "4    2033        0.990935\n"
     ]
    }
   ],
   "source": [
    "CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"config_A\",\n",
    "        \"embed_model_keys\": [\"bge-m3\", \"all-mpnet-base-v2\", 'qwen-3'],\n",
    "        \"learners\": [\"lr\", \"lgbm\"],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"config_B\",\n",
    "        \"embed_model_keys\": [\"all-minilm-l6-v2\"],\n",
    "        \"learners\": [\"nb\", \"ridge\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "all_members = []\n",
    "for i, cfg in enumerate(CONFIGS):\n",
    "    all_members.extend(run_config(cfg, random_state=42 + i))\n",
    "\n",
    "print(\"\\n=== Final Ensemble Blending ===\")\n",
    "aucs = np.array([m[\"oof_auc\"] for m in all_members])\n",
    "weights = np.maximum(aucs, 1e-6) ** WEIGHT_POWER\n",
    "weights /= weights.sum()\n",
    "\n",
    "for i, m in enumerate(all_members):\n",
    "    print(f\"Member '{m['name']}': OOF AUC={m['oof_auc']:.5f}, Weight={weights[i]:.4f}\")\n",
    "\n",
    "test_stack = np.vstack([m[\"test_preds\"] for m in all_members])\n",
    "y_pred_ensemble = np.average(test_stack, axis=0, weights=weights)\n",
    "\n",
    "# --- Create Submission File ---\n",
    "df_sub = pd.read_csv(TEST_PATH)[[\"row_id\"]]\n",
    "df_sub[\"rule_violation\"] = y_pred_ensemble\n",
    "df_sub.to_csv(OUT_ENSEMBLE_CSV, index=False)\n",
    "\n",
    "print(f\"\\n Ensemble submission written to: {OUT_ENSEMBLE_CSV}\")\n",
    "print(\"Submission Head:\")\n",
    "print(df_sub.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "datasetId": 8472990,
     "sourceId": 13358287,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8473668,
     "sourceId": 13359403,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 78074,
     "modelInstanceId": 56578,
     "sourceId": 67859,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 95613,
     "modelInstanceId": 70561,
     "sourceId": 84013,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 368803,
     "modelInstanceId": 347541,
     "sourceId": 426330,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 368803,
     "modelInstanceId": 347543,
     "sourceId": 426333,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 457105,
     "modelInstanceId": 440562,
     "sourceId": 589155,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
